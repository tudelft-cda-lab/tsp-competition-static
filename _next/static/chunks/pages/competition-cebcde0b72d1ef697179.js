_N_E=(window.webpackJsonp_N_E=window.webpackJsonp_N_E||[]).push([[10],{"8TM4":function(e,t,i){"use strict";i.r(t),i.d(t,"default",(function(){return l}));var n=i("nKUr"),s=i("g4pe"),r=i.n(s),c=i("ZHh6"),a=i("7vrA"),o=i("xXt2");i("YFqc");function l(){return Object(n.jsxs)("div",{children:[Object(n.jsxs)(r.a,{children:[Object(n.jsx)("title",{children:"AI for TSP competition"}),Object(n.jsx)("link",{rel:"icon",href:"img/icon.png"})]}),Object(n.jsx)(c.a,{}),Object(n.jsxs)("div",{className:"blur-bg",children:[Object(n.jsx)(o.a,{fluid:!0,children:Object(n.jsxs)(a.a,{children:[Object(n.jsx)("h1",{children:"Competition"}),Object(n.jsx)("p",{children:"In this competition, participants will solve a variant of TSP that is particularly difficult for traditional optimization algorithms, using different methods depending on the track: online supervised learning (track 1) or reinforcement learning (track 2). The competition is sponsored by Ortec and Vanderlande."}),Object(n.jsxs)("p",{children:["For more details about the competition, please refer to ",Object(n.jsx)("a",{href:"TSP_Competition.pdf",children:" this document "})," and check out our ",Object(n.jsx)("a",{href:"https://github.com/paulorocosta/ai-for-tsp-competition",children:"github repository."})]})]})}),Object(n.jsxs)(a.a,{children:[Object(n.jsx)("div",{className:"row",children:Object(n.jsxs)("div",{className:"col-lg",children:[Object(n.jsx)("h1",{id:"tracks",children:"Tracks"}),Object(n.jsx)("p",{children:"The competition is split into two tracks that are scored separately. Prizes will be awarded to the first and second places in both tracks."})]})}),Object(n.jsxs)("div",{className:"row",children:[Object(n.jsxs)("div",{className:"col-lg",children:[Object(n.jsx)("h2",{id:"track-1-online-supervised-learning-surrogates",children:"Track 1: Online supervised learning (surrogates)"}),Object(n.jsx)("p",{children:"Problem: time-dependent orienteering problem with stochastic weights and time windows (TD-OPSWTW) [1]. Given one instance, previously tried routes, and the reward for those routes, the goal is to learn a model that can predict the reward for a new route. Then an optimizer finds the route that gives the best reward according to that model, and that route is then evaluated, giving a new data point. Then the model is updated, and this iterative procedure continues for a fixed number of steps."}),Object(n.jsxs)("p",{children:[Object(n.jsx)("a",{href:"https://www.sciencedirect.com/science/article/pii/S037722171630368X",children:"[1]"})," C Verbeeck, Pieter Vansteenwegen, and E-H Aghezzaf. Solving the stochastic time-dependent orienteering problem with time windows. European Journal of Operational Research, 255(3):699\u2013718, 2016."]})]}),Object(n.jsxs)("div",{className:"col-lg",children:[Object(n.jsx)("h2",{id:"track-2-reinforcement-learning",children:"Track 2: Reinforcement learning"}),Object(n.jsx)("p",{children:"The problem is the same as in track 1, but for multiple instances. We consider an environment (simulator) that can generate multiple instances following the same distribution and expects as output (partial) solutions containing the order at which the nodes should be visited. The simulator returns general instance features and the time-dependent cost for traversing the last edge in a given solution. The goal is to minimize the cost of the total path over multiple samples of selected test instances."})]})]})]}),Object(n.jsx)(o.a,{fluid:!0,children:Object(n.jsxs)(a.a,{children:[Object(n.jsx)("h1",{id:"submitting-results",children:"Submitting results"}),Object(n.jsx)("p",{children:"The competition is closed. We hope you will join next time!"})]})}),Object(n.jsx)(a.a,{children:Object(n.jsxs)("div",{className:"row",children:[Object(n.jsxs)("div",{className:"col-lg",children:[Object(n.jsx)("h1",{id:"timeline",children:"Timeline"}),Object(n.jsxs)("ul",{children:[Object(n.jsx)("li",{children:"May 7: Start of test period"}),Object(n.jsx)("li",{children:"May 21: Competition start"}),Object(n.jsx)("li",{children:"July 5: Submission deadline (validation)"}),Object(n.jsx)("li",{children:"July 12: Submission deadline (test)"}),Object(n.jsx)("li",{children:"August 9: Winners are contacted privately"}),Object(n.jsx)("li",{children:"August 21/22: Public announcement of winners"})]})]}),Object(n.jsxs)("div",{className:"col-lg",children:[Object(n.jsx)("h1",{id:"prizes",children:"Prizes"}),Object(n.jsx)("p",{children:"The total prize money is 1000\u20ac"}),Object(n.jsxs)("ul",{children:[Object(n.jsx)("li",{children:"First place (both tracks): 350\u20ac"}),Object(n.jsx)("li",{children:"Second place (both tracks): 150\u20ac"})]}),Object(n.jsx)("p",{children:"We will also sponsor one IJCAI workshop registration per invited team for the winners who want to present their work at the Data Science meets Optimization workshop."})]})]})})]})]})}},VCth:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/competition",function(){return i("8TM4")}])},xXt2:function(e,t,i){"use strict";var n=i("RAs/"),s=i("hVfy"),r=i("q1tI"),c=i.n(r),a=i("eC2I"),o=i.n(a),l=i("vUet"),d=["as","className","fluid","bsPrefix"],h=c.a.forwardRef((function(e,t){var i,r=e.as,a=void 0===r?"div":r,h=e.className,j=e.fluid,p=e.bsPrefix,u=Object(s.a)(e,d),b=((i={})[p=Object(l.a)(p,"jumbotron")]=!0,i[p+"-fluid"]=j,i);return c.a.createElement(a,Object(n.a)({ref:t},u,{className:o()(h,b)}))}));h.defaultProps={fluid:!1},h.displayName="Jumbotron",t.a=h}},[["VCth",0,2,1,3,4,5]]]);